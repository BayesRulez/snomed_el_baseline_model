{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c252ef-6591-48d7-b7ab-1dd520a02d17",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Much of the world's healthcare data is stored in free-text documents, usually clinical notes taken by doctors. This unstructured data can be challenging to analyze and extract meaningful insights from. However, by applying a standardized terminology like SNOMED CT, healthcare organizations can convert this free-text data into a structured format that can be readily analyzed by computers, in turn stimulating the development of new medicines, treatment pathways, and better patient outcomes.\n",
    "\n",
    "One way to analyze clinical notes is to identify and label the portions of each note that correspond to specific medical concepts. This process is called entity linking because it involves identifying candidate spans in the unstructured text (the entities) and linking them to a particular concept in a knowledge base of medical terminology.\n",
    "\n",
    "However, clinical entity linking is hard!  Medical notes are often rife with abbreviations (some of them context-dependent) and assumed knowledge. Furthermore, the target knowledge bases can easily include hundreds of thousands of concepts, many of which occur infrequently leading to a “long tail” effect in the distribution of concepts.\n",
    "\n",
    "The objective of the competition is to link spans of text in clinical notes with specific topics in the SNOMED CT clinical terminology. Participants will train models based on real-world doctors' notes which have been de-identified and annotated with SNOMED CT concepts by medically trained professionals.\n",
    "\n",
    "In this post, we build a straightforward entity linking model and prepare it for submission.  \n",
    "\n",
    "Typically, an entity linker contains two components:\n",
    "\n",
    "- The Clinical Entity Recognizer (CER) model is responsible for detecting candidate clinical entities from within the text.\n",
    "- The Linker is responsible for connecting the entities to the knowledge base.  Often (as here) the liner's tasks are split into two steps:\n",
    "    - In the Candidate Generation step, the Linker retrieves a handful of candidate concepts that it thinks may match to the entity.\n",
    "    - In the Candidate Selection step, the linker selects the best candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dec53e4-02be-4e99-9165-091942597bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snomed_graph import *\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from more_itertools import chunked\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import combinations\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer, models, InputExample, losses\n",
    ")\n",
    "from ipymarkup import show_span_line_markup\n",
    "from peft import PeftConfig, PeftModel, LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from collections import Counter\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import dill as pickle\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    pipeline,\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForTokenClassification,\n",
    "    DebertaV2ForTokenClassification \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c3b83a-bab1-453c-9321-ecfd30f993a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42                                                    # For reproducibility\n",
    "max_seq_len = 512                                                   # Maximum sequence length for (BERT-based) encoders \n",
    "cer_model_id = \"microsoft/deberta-v3-large\"                         # Base model for Clinical Entity Recogniser\n",
    "kb_embedding_model_id = \"sentence-transformers/all-MiniLM-L6-v2\"    # base model for concept encoder\n",
    "use_LoRA = False                                                    # Whether to use a LoRA to fine-tune the CER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c51114be-b3f8-4586-8c57-a4072b32f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598d4eb0-8721-4aff-b908-30fee696023c",
   "metadata": {},
   "source": [
    "# 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a6a22c4-617a-42e0-b9b1-dcc5d2c6b685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204 notes loaded.\n"
     ]
    }
   ],
   "source": [
    "notes_df = (\n",
    "    pd.read_csv(\"data/training_notes.csv\")\n",
    "    .set_index(\"note_id\")\n",
    ")\n",
    "print(f\"{notes_df.shape[0]} notes loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de7506cc-d532-49f2-89b7-aaea55d06c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51574 annotations loaded.\n",
      "5336 unique concepts seen.\n",
      "204 unique notes seen.\n"
     ]
    }
   ],
   "source": [
    "annotations_df = (\n",
    "    pd.read_csv(\"data/training_annotations.csv\")\n",
    "    .set_index(\"note_id\")\n",
    ")\n",
    "print(f\"{annotations_df.shape[0]} annotations loaded.\")\n",
    "print(f\"{annotations_df.concept_id.nunique()} unique concepts seen.\")\n",
    "print(f\"{annotations_df.index.nunique()} unique notes seen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f64137-236d-43e4-abda-4ba3e290c527",
   "metadata": {},
   "source": [
    "## 1.1 Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35e57731-4b49-41b3-8bff-45e2ab76e642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 44075 total annotations in the training set.\n",
      "There are 7499 total annotations in the test set.\n",
      "There are 4924 distinct concepts in the training set.\n",
      "There are 1799 distinct concepts in the test set.\n",
      "There are 172 notes in the training set.\n",
      "There are 32 notes in the test set.\n"
     ]
    }
   ],
   "source": [
    "training_notes_df, test_notes_df = train_test_split(notes_df, test_size=32, random_state=random_seed)\n",
    "training_annotations_df = annotations_df.loc[training_notes_df.index]\n",
    "test_annotations_df = annotations_df.loc[test_notes_df.index]\n",
    "\n",
    "print(f\"There are {training_annotations_df.shape[0]} total annotations in the training set.\")\n",
    "print(f\"There are {test_annotations_df.shape[0]} total annotations in the test set.\")\n",
    "print(f\"There are {training_annotations_df.concept_id.nunique()} distinct concepts in the training set.\")\n",
    "print(f\"There are {test_annotations_df.concept_id.nunique()} distinct concepts in the test set.\")\n",
    "print(f\"There are {training_notes_df.shape[0]} notes in the training set.\")\n",
    "print(f\"There are {test_notes_df.shape[0]} notes in the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bdcdff-cabd-4012-a85a-90fd102bdd32",
   "metadata": {},
   "source": [
    "# 2. Train the CER model\n",
    "\n",
    "This will be a token classifier, based on the widely-used BERT architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731650a7-fca9-4cf1-be5a-aae9ed42e3b5",
   "metadata": {},
   "source": [
    "## 2.1 Define the token types\n",
    "\n",
    "A token classifier is typically looking to tag tokens according to the part of speech or entity type.  We have quite a simple task here: locate tokens that are part of clinical entities.  We define the following token labels:\n",
    "\n",
    "- *O*.  This token is not part of an entity.\n",
    "- *B-clinical_entity*. This token is the beginning (first part of the first word) of a clinical entity.\n",
    "- *I-clinical_entity*. This token is inside a clinical entity - i.e. not the first word but a subsequent word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65ab477a-c300-4368-8976-a4addc397041",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    'O': 0, \n",
    "    'B-clinical_entity': 1, \n",
    "    'I-clinical_entity': 2\n",
    "}\n",
    "\n",
    "id2label = {v: k for k,v in label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ead1df-a752-4316-99fd-45650e52c8e7",
   "metadata": {},
   "source": [
    "## 2.2 Load a tokenizer\n",
    "\n",
    "We'll use the tokenizer for our chosen NER model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1ca0f45-4afa-4dbd-9c84-25db15d78d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/willh/venvs/snomed/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cer_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cer_model_id, \n",
    "    model_max_length=max_seq_len\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf1bc37-6fb0-4207-9a7f-d6a690d513e1",
   "metadata": {},
   "source": [
    "## 2.3 Construct training and test datasets for the CER model\n",
    "\n",
    "The annotation dataset contains tuples of the form `(note_id, concept_id, start, end)`.\n",
    "\n",
    "To create a dataset for the token classifier we need to make two transformations to the data:\n",
    "\n",
    "1. We have to split the discharge notes into chunks of 512 characters (the input dimension for BERT-based models).\n",
    "2. We have to tokenize the discharge notes and determine which of the resulting tokens fall within the span of an annotation according to the `label2id` map defined above.\n",
    "\n",
    "We will create a dataset consisting of 512-token chunks, along with a length-512 vector flagging the tokens which appear within an annotation.\n",
    "\n",
    "One further consideration is that the tokenizer will tokenize to a sub-word level.  For example, this tokenizer will split the word `tokenization` into two sub-words: `__token` and `ization`.  We will always flag the first token of each word with the appropriate entity type (\"B\", \"I\" or \"O\") but need to decide how to flag subsequent sub-words.  One way is to flag these with a `-100` value, which is interpreted used by `pytorch` loss functions as \"ignore this value\".  This involves complicating the alignment logic, however.  Instead, the approach taken below is to flag all subwords with the appropriate \"I\" or \"B\" label.  (The tokenizer offers a handy `word_ids()` function which we can use to determine whether a particular token represents the start of a new word or the continuation of the previous word.)\n",
    "\n",
    "The logic for the CER tokenizer is therefore as follows:\n",
    "\n",
    "- First token of the first word within an annotation: `B-clinical_entity`\n",
    "- First token a subsequent word within an annotation: `I-clinical_entity`\n",
    "- First token of a word not within an annotation: `O`\n",
    "- Special token ([CLS], [SEP]): `-100`\n",
    "\n",
    "The first token of an input to a BERT-based model must be the classificiation (`[CLS]`) token and the last must be the separator (`[SEP]`).  We add these manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9220c9e7-e516-4a43-a67a-68e117ce2648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step through the annotation spans for a given note.  When they're exhausted,\n",
    "# return (1000000, 1000000).  This will avoid a StopIteration exception.\n",
    "\n",
    "def get_annotation_boundaries(note_id, annotations_df):\n",
    "    for row in annotations_df.loc[note_id].itertuples():\n",
    "        yield row.start, row.end, row.concept_id\n",
    "    yield 1000000, 1000000, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b802c41-c760-4c2d-ab91-dca044b2e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ner_dataset(notes_df, annotations_df):\n",
    "\n",
    "    for row in notes_df.itertuples():\n",
    "        \n",
    "        tokenized = cer_tokenizer(\n",
    "            row.text, \n",
    "            return_offsets_mapping=False,   # Avoid misalignments due to destructive tokenization\n",
    "            return_token_type_ids=False,    # We're going to construct these below\n",
    "            return_attention_mask=False,    # We'll construct this by hand\n",
    "            add_special_tokens=False,       # We'll add these by hand\n",
    "            truncation=False,               # We'll chunk the notes ourselves\n",
    "        )\n",
    "\n",
    "        # Prime the annotation generator and fetch the token <-> word_id map\n",
    "        annotation_boundaries = get_annotation_boundaries(row.Index, annotations_df)\n",
    "        ann_start, ann_end, concept_id = next(annotation_boundaries)\n",
    "        word_ids = tokenized.word_ids()\n",
    "\n",
    "        # The offsets_mapping returned by the tokenizer will be misaligned vs the original text.\n",
    "        # This is due to the fact that the tokenization scheme is destructive, for example it \n",
    "        # drops spaces which cannot be recovered when decoding the inputs.\n",
    "        # In the following code snippet we create an offset mapping which is aligned with the \n",
    "        # original text; hence we can accurately locate the annotations and match them to the\n",
    "        # tokens.\n",
    "        global_offset = 0\n",
    "        global_offset_mapping = []\n",
    "        \n",
    "        for input_id in tokenized[\"input_ids\"]:\n",
    "            token = cer_tokenizer.decode(input_id)\n",
    "            pos = row.text[global_offset:].find(token)\n",
    "            start = global_offset + pos\n",
    "            end = global_offset + pos + len(token)\n",
    "            global_offset = end\n",
    "            global_offset_mapping.append((start, end))        \n",
    "\n",
    "        # Note the max_seq_len - 2.\n",
    "        # This is because we will have to add [CLS] and [SEP] tokens once we're done.\n",
    "        it = zip(\n",
    "            chunked(tokenized[\"input_ids\"], max_seq_len-2),\n",
    "            chunked(global_offset_mapping, max_seq_len-2),\n",
    "            chunked(word_ids, max_seq_len-2)\n",
    "        )\n",
    "\n",
    "        # Since we are chunking the discharge notes, we need to maintain the start and\n",
    "        # end character index for each chunk so that we can align the annotations for\n",
    "        # chunks > 1\n",
    "        chunk_start_idx = 0\n",
    "        chunk_end_idx = 0\n",
    "        \n",
    "        for chunk_id, chunk in enumerate(it):\n",
    "            input_id_chunk, offset_mapping_chunk, word_id_chunk = chunk\n",
    "            token_type_chunk = list()\n",
    "            concept_id_chunk = list()\n",
    "            prev_word_id = -1\n",
    "            concept_word_number = 0\n",
    "            chunk_start_idx = chunk_end_idx\n",
    "            chunk_end_idx = offset_mapping_chunk[-1][1]\n",
    "            \n",
    "            for offsets, word_id in zip(offset_mapping_chunk, word_id_chunk):\n",
    "                token_start, token_end = offsets\n",
    "                \n",
    "                # Check whether we need to fetch the next annotation\n",
    "                if token_start >= ann_end:\n",
    "                    ann_start, ann_end, concept_id = next(annotation_boundaries)  \n",
    "                    concept_word_number = 0\n",
    "            \n",
    "                # Check whether the token's position overlaps with the next annotation\n",
    "                if token_start < ann_end and token_end > ann_start:\n",
    "\n",
    "                    if prev_word_id != word_id:\n",
    "                        concept_word_number += 1\n",
    "                    \n",
    "                    # If so, annotate based on the word number in the concept\n",
    "                    if concept_word_number == 1:\n",
    "                        token_type_chunk.append(label2id[\"B-clinical_entity\"])\n",
    "                    else:\n",
    "                        token_type_chunk.append(label2id[\"I-clinical_entity\"])\n",
    "\n",
    "                    # Add the SCTID (we'll use this later to train the Linker)\n",
    "                    concept_id_chunk.append(concept_id)\n",
    "        \n",
    "                # Not part of an annotation\n",
    "                else:\n",
    "                    token_type_chunk.append(label2id[\"O\"])\n",
    "                    concept_id_chunk.append(None)\n",
    "            \n",
    "                prev_word_id = word_id\n",
    "\n",
    "            # Manually adding the [CLS] and [SEP] tokens.\n",
    "            token_type_chunk = [-100] + token_type_chunk + [-100]\n",
    "            input_id_chunk = [cer_tokenizer.cls_token_id] + input_id_chunk + [cer_tokenizer.sep_token_id]\n",
    "            attention_mask_chunk = [1] * len(input_id_chunk)\n",
    "            offset_mapping_chunk = [(None, None)] + offset_mapping_chunk + [(None, None)]\n",
    "            concept_id_chunk = [None] + concept_id_chunk + [None]\n",
    "            \n",
    "            yield {\n",
    "                # These are the fields we need\n",
    "                \"note_id\": row.Index,\n",
    "                \"input_ids\": input_id_chunk,\n",
    "                \"attention_mask\": attention_mask_chunk,\n",
    "                \"labels\": token_type_chunk,\n",
    "                # These fields are helpful for debugging\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"chunk_span\": (chunk_start_idx, chunk_end_idx),\n",
    "                \"offset_mapping\": offset_mapping_chunk,\n",
    "                \"text\": row.text[chunk_start_idx : chunk_end_idx],                \n",
    "                \"concept_ids\": concept_id_chunk,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "382fcd88-0ccd-4fd9-bffd-e9b45cbb62f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1629 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['note_id', 'input_ids', 'attention_mask', 'labels', 'chunk_id', 'chunk_span', 'offset_mapping', 'text', 'concept_ids'],\n",
       "    num_rows: 920\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can ignore the \"Token indices sequence length is longer than the specified maximum sequence length\"\n",
    "# warning because we are chunking by hand.\n",
    "train = pd.DataFrame(list(generate_ner_dataset(training_notes_df, training_annotations_df)))\n",
    "train = Dataset.from_pandas(train)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80c02046-025b-4c13-9359-78f11b398664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['note_id', 'input_ids', 'attention_mask', 'labels', 'chunk_id', 'chunk_span', 'offset_mapping', 'text', 'concept_ids'],\n",
       "    num_rows: 152\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.DataFrame(list(generate_ner_dataset(test_notes_df, test_annotations_df)))\n",
    "test = Dataset.from_pandas(test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "030d7dd8-62f6-4d1b-bcda-64157fce7610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data collator handles batching for us.\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=cer_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07519dc5-5217-43af-8246-9808f3b6bc3f",
   "metadata": {},
   "source": [
    "## 2.4 Define some training metrics for the fine-tuning run\n",
    "\n",
    "It's always easier to be able to track some meaningful performance metrics during a training run, rather than simple watching a cross-entropy loss function change.  This is a standard, boilerplate function taken directly from a HuggingFace tutorial that is useful for any classifier fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56d6d2b0-2c08-444b-83c4-56684577a005",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f4cb30-2538-47b0-b3d0-985f40c22330",
   "metadata": {},
   "source": [
    "## 2.5 Define and train the model\n",
    "\n",
    "The `deberta-v3-large` model (model card: https://huggingface.co/microsoft/deberta-v3-large) has 304M parameters.  To speed up the fine-tuning can use a LoRA, which will greatly reduce the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f6d8b3-78be-48eb-914f-dcbcfa9b347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cer_model = DebertaV2ForTokenClassification.from_pretrained(\n",
    "    cer_model_id, \n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")   \n",
    "\n",
    "if use_LoRA:\n",
    "    lora_config = LoraConfig(\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.1,\n",
    "        r=8,\n",
    "        bias=\"none\",\n",
    "        task_type=\"TOKEN_CLS\",\n",
    "    )\n",
    "    \n",
    "    cer_model = get_peft_model(cer_model, lora_config)\n",
    "    \n",
    "    cer_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a8f02d5-9817-4ee8-a787-563099dd730d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='115' max='115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [115/115 23:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>0.174124</td>\n",
       "      <td>0.796415</td>\n",
       "      <td>0.831569</td>\n",
       "      <td>0.813613</td>\n",
       "      <td>0.938818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=115, training_loss=0.11113357336624809, metrics={'train_runtime': 1407.5783, 'train_samples_per_second': 0.654, 'train_steps_per_second': 0.082, 'total_flos': 854419117793280.0, 'train_loss': 0.11113357336624809, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"~/temp/cer_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=random_seed\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=cer_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    tokenizer=cer_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7982c37b-9bde-4bd1-8605-c1599de1df1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cer_model/tokenizer_config.json',\n",
       " 'cer_model/special_tokens_map.json',\n",
       " 'cer_model/spm.model',\n",
       " 'cer_model/added_tokens.json',\n",
       " 'cer_model/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"cer_model\")\n",
    "cer_tokenizer.save_pretrained(\"cer_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b60713-16ab-4016-a620-8b36a0cdce86",
   "metadata": {},
   "source": [
    "## 2.6 CER Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b3071dc-db59-4540-9e47-037cde8aea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can ignore the warning message.  This is simply due to the fact that\n",
    "# DebertaV2ForTokenClassification loads the DebertaV2 model first, then \n",
    "# initializes a random header model before restoring the states of the \n",
    "# TokenClassifer.  So we *do* have our fine-tuned model available. \n",
    "\n",
    "if use_LoRA:\n",
    "    config = PeftConfig.from_pretrained(\"cer_model\")\n",
    "\n",
    "    cer_model = DebertaV2ForTokenClassification.from_pretrained(\n",
    "        pretrained_model_name_or_path=config.base_model_name_or_path,\n",
    "        num_labels=3, \n",
    "        id2label=id2label, \n",
    "        label2id=label2id\n",
    "    )  \n",
    "    cer_model = PeftModel.from_pretrained(cer_model, \"cer_model\")\n",
    "else:\n",
    "    cer_model = DebertaV2ForTokenClassification.from_pretrained(\n",
    "        pretrained_model_name_or_path=\"cer_model\",\n",
    "        num_labels=3, \n",
    "        id2label=id2label, \n",
    "        label2id=label2id\n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cdff28c-5c01-49a2-89ef-af6303408004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using the adaptor, ignore the warning: \n",
    "# \"The model 'PeftModelForTokenClassification' is not supported for token-classification.\"\n",
    "# The PEFT model is wrapped just fine and will work within the pipeline.\n",
    "# N.B. moving model to CPU makes inference slower, but enables us to feed the pipeline \n",
    "# directly with strings.\n",
    "cer_pipeline = pipeline(\n",
    "    task=\"token-classification\", \n",
    "    model=cer_model, \n",
    "    tokenizer=cer_tokenizer, \n",
    "    aggregation_strategy=\"first\",\n",
    "    device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f97a67-e3ad-4b53-aa11-b3af5e19c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the predicted clinical entities against the actual annotated entities.\n",
    "# N.B. only the first 512 tokens of the note will contain predicted spans.\n",
    "# Not run due to sensitivity of MIMIC-IV notes\n",
    "\n",
    "note_id = \"10807423-DS-19\"\n",
    "text = test_notes_df.loc[note_id].text\n",
    "\n",
    "predicted_annotations = [\n",
    "    (span[\"start\"], span[\"end\"], \"PRED\") for span in cer_pipeline(text)\n",
    "]\n",
    "\n",
    "gt_annotations = [\n",
    "    (row.start, row.end, \"GT\")\n",
    "    for row in test_annotations_df.loc[note_id].itertuples()\n",
    "]\n",
    "\n",
    "show_span_line_markup(text, predicted_annotations + gt_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9fc3b3-0a0a-41af-85fd-59f6d10d6ced",
   "metadata": {},
   "source": [
    "# 3. Linking Model\n",
    "\n",
    "The second part of the Entity Linker is the Linking model.  This component is charged with selecting the concepts from the knowledge base that best match the detected entity.\n",
    "\n",
    "We will build a simple, multi-level indexer for the task, drawing upon an encoder-only transformer that has been fine-tuned across the SNOMED CT concepts.\n",
    "\n",
    "The first index will find the most similar entity seen during training.  The second will use the context surrounding the entity to find the most likely concept matching said entity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28b7114-6ccb-4cfe-a60c-5edac4dd1233",
   "metadata": {},
   "source": [
    "## 3.1 Load the knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd46712-d04d-4e0b-a9ee-fc2e27822baa",
   "metadata": {},
   "source": [
    "To load from a SNOMED RF2 folder, use:\n",
    "\n",
    "```SG = SnomedGraph.from_rf2(\"SnomedCT_InternationalRF2_PRODUCTION_20230531T120000Z\")```\n",
    "\n",
    "Here, we will load a previously constructed concept graph and filter to the concepts that were in scope of the annotation exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "178b9c70-60ef-4932-8628-05c73e7f2d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNOMED graph has 361179 vertices and 1179749 edges\n"
     ]
    }
   ],
   "source": [
    "SG = SnomedGraph.from_serialized(\"../snomed_graph/full_concept_graph.gml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d083491e-93d4-45fb-8951-c9a406b5f781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If we want to load all of the concepts that were in scope of the annotation exercise, it's this:\n",
    "concepts_in_scope = SG.get_descendants(71388002) | SG.get_descendants(123037004) | SG.get_descendants(404684003)\n",
    "print(f\"{len(concepts_in_scope)} concepts have been selected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f435176-2630-42cf-9875-5d2610900a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5336 concepts have been selected.\n"
     ]
    }
   ],
   "source": [
    "# If we want to simply use concepts for which we have a training example, it's this:\n",
    "concepts_in_scope = [\n",
    "    SG.get_concept_details(a)\n",
    "    for a in annotations_df.concept_id.unique()\n",
    "]\n",
    "\n",
    "print(f\"{len(concepts_in_scope)} concepts have been selected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e530662-c438-4812-ae66-24392922f667",
   "metadata": {},
   "source": [
    "## 3.2 Fine-tune the Linker's Encoder\n",
    "\n",
    "To fine-tune the encoder, we'll collect each in-scope concept from SNOMED CT and generate a training example from each pairwise combination of synonyms.  We train with a multiple negative-rankings loss.  This calculates the distance between each example pair and also the distance between the first example in the pair and _all other_ first examples in the batch.  The loss is constructed from the ranking of these distances.  We want the distance between an example and itself to be the minimum of all distances in the batch.  This should result in an embedding in which synonyms for the SNOMED concepts are encoded into close proximity.\n",
    "\n",
    "Note that this is a relatively trivial exploitation of the SNOMED CT graph.  We could experiment with other ways to generate pairs too, for example: by generating pairs that consist of parent and child concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465e5782-d5db-44bb-bd0d-3542777bdc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_model = SentenceTransformer(kb_embedding_model_id)\n",
    "\n",
    "kb_sft_examples = [\n",
    "    InputExample(texts=[syn1, syn2], label=1)\n",
    "    for concept in tqdm(concepts_in_scope)\n",
    "    for syn1, syn2 in combinations(concept.synonyms, 2)\n",
    "]\n",
    "\n",
    "kb_sft_dataloader = DataLoader(kb_sft_examples, shuffle=True, batch_size=32)\n",
    "\n",
    "kb_sft_loss = losses.ContrastiveLoss(kb_model)\n",
    "\n",
    "kb_model.fit(\n",
    "    train_objectives=[(kb_sft_dataloader, kb_sft_loss)], \n",
    "    epochs=2, \n",
    "    warmup_steps=100,\n",
    "    checkpoint_path=\"~/temp/ke_encoder\",\n",
    ")\n",
    "\n",
    "kb_model.save(\"kb_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38657f44-b085-4a1a-94d6-17ad055cd938",
   "metadata": {},
   "source": [
    "## 3.3 Construct the Linker\n",
    "\n",
    "The simplest linker would simply map an entity (as extracted by the CER model) to the associated concept in the training dataset.  Two problems with this approach present themselves:\n",
    "\n",
    "1. We might encounter entities that have not been seen during training.\n",
    "2. Some entities might be mapped to >1 concept.  Why would this happen?  Consider the entity \"ABD\".  This is an abbreviation for \"Acute behavioural disorder\".  However, it is also shorthand for \"Abdomen\".\n",
    "\n",
    "To resolve the first problem our linker keeps an index of entities seen during training.  At inference time, it selects the known entity that is closest to the entity it is presented with.  (This is the \"candidate generation\" step.)\n",
    "\n",
    "To resolve the second problem, the linker builds a \"second level\" index for each entity.  This second level index maps each occurance of an entity + its surrounding context to the SNOMED concept it was annotated with.  At inference time, we encode the \\[entity + context\\] and find the most similar result in the second level index.  We return the associated SCTID.  (This is the \"candidate selection\" step.)\n",
    "\n",
    "We perform a simple grid search over context window sizes.\n",
    "\n",
    "As a further enhancement, we not only train the linker using entities seen in the training dataset but also with all of the synonyms for the in-scope SNOMED concepts (here there is no \"context\" for each of the entities, so we simply use the entity as it's own context.)  You can run an ablation experiment by not passing the Linker any SNOMED concepts.  The performance will drop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2efd6288-c35c-4889-b7f9-fe0d3471a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linker():\n",
    "    \n",
    "    def __init__(self, encoder, context_window_width=0):\n",
    "        self.encoder = encoder\n",
    "        self.entity_index = KeyedVectors(self.encoder[1].word_embedding_dimension)\n",
    "        self.context_index = dict()\n",
    "        self.history = dict()\n",
    "        self.context_window_width = context_window_width\n",
    "\n",
    "    def add_context(self, row):\n",
    "        window_start = max(0, row.start-self.context_window_width)\n",
    "        window_end = min(row.end+self.context_window_width, len(row.text))\n",
    "        return row.text[window_start : window_end]\n",
    "\n",
    "    def add_entity(self, row):\n",
    "        return row.text[row.start : row.end]       \n",
    "\n",
    "    def fit(self, df=None, snomed_concepts=None):\n",
    "        # Create a map from the entities to the concepts and contexts in which they appear\n",
    "        if df is not None:\n",
    "            for row in df.itertuples():\n",
    "                entity = self.add_entity(row)\n",
    "                context = self.add_context(row)\n",
    "                map_ = self.history.get(entity, dict())\n",
    "                contexts = map_.get(row.concept_id, list())\n",
    "                contexts.append(context)\n",
    "                map_[row.concept_id] = contexts\n",
    "                self.history[entity] = map_\n",
    "\n",
    "        # Add SNOMED CT codes for lookup\n",
    "        if snomed_concepts is not None:\n",
    "            for c in snomed_concepts:\n",
    "                for syn in c.synonyms:\n",
    "                    map_ = self.history.get(syn, dict())\n",
    "                    contexts = map_.get(c.sctid, list())\n",
    "                    contexts.append(syn)\n",
    "                    map_[c.sctid] = contexts\n",
    "                    self.history[syn] = map_            \n",
    "            \n",
    "        # Create indexes to help disambiguate entities by their contexts\n",
    "        for entity, map_ in tqdm(self.history.items()):\n",
    "            keys = [\n",
    "                (concept_id, occurance)\n",
    "                for concept_id, contexts in map_.items()\n",
    "                for occurance, context in enumerate(contexts)\n",
    "            ]\n",
    "            contexts = [\n",
    "                context \n",
    "                for contexts in map_.values() \n",
    "                for context in contexts\n",
    "            ]\n",
    "            vectors = self.encoder.encode(contexts)\n",
    "            index = KeyedVectors(self.encoder[1].word_embedding_dimension)\n",
    "            index.add_vectors(keys, vectors)\n",
    "            self.context_index[entity] = index\n",
    "\n",
    "        # Now create the top-level entity index\n",
    "        keys = list(self.history.keys())\n",
    "        vectors = self.encoder.encode(keys)\n",
    "        self.entity_index.add_vectors(keys, vectors)\n",
    "\n",
    "    def link(self, row):\n",
    "        entity = self.add_entity(row)\n",
    "        context = self.add_context(row)        \n",
    "        vec = self.encoder.encode(entity)\n",
    "        nearest_entity = self.entity_index.most_similar(vec, topn=1)[0][0]     \n",
    "        index = self.context_index.get(nearest_entity, None)\n",
    "        \n",
    "        if index:\n",
    "            vec = self.encoder.encode(context)\n",
    "            key, score = index.most_similar(vec, topn=1)[0]\n",
    "            sctid, _ = key\n",
    "            return sctid\n",
    "        else:\n",
    "            return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a2d7a4c-2aaa-4da4-8a4d-bac0c57d27e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "linker_training_df = training_notes_df.join(training_annotations_df)\n",
    "linker_test_df = test_notes_df.join(test_annotations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54ad2e8c-85db-47c8-8d09-7e10bd10b281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_linker(linker, df):\n",
    "    n_correct = 0\n",
    "    n_examples = df.shape[0]\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=n_examples):\n",
    "        sctid = linker.link(row)\n",
    "        if row[\"concept_id\"] == sctid:\n",
    "            n_correct += 1\n",
    "    \n",
    "    return n_correct / n_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c82e6c6b-f46b-46d6-9670-efd5685e6fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2cb5ffe37f8465bb7822e416dac4ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8567809041205494"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_linker(linker, linker_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b969fc37-2dc8-4976-9b79-f250f5a9effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for context_window_width in tqdm([5, 10, 15, 20, 25, 30]):\n",
    "    linker = Linker(kb_model, context_window_width)\n",
    "    linker.fit(linker_training_df, concepts_in_scope)\n",
    "    acc = evaluate_linker(linker, linker_test_df)\n",
    "    print(f\"Context Window Width: {context_window_width}\\tAccuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abf7ee6-e89a-4f74-b043-a0bbeb186a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "linker = Linker(kb_model, 10)\n",
    "linker.fit(linker_training_df, concepts_in_scope)\n",
    "\n",
    "with open(\"linker.pickle\", \"wb\") as f:\n",
    "    pickle.dump(linker, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c81ef28-4bf6-4232-b081-9932eda39078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then re-load the linker with:\n",
    "with open(\"linker.pickle\", \"rb\") as f:\n",
    "    linker = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4c7f6a-d395-4f00-ba2c-64e3d3167692",
   "metadata": {},
   "source": [
    "# 4. Evaluation\n",
    "\n",
    "Here we glue the Clinical Entity Recogniser model to the Linker model and show how to generate and evaluate predictions over our test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5123c61-35db-4110-a485-0d7737f403a8",
   "metadata": {},
   "source": [
    "## 4.1 Prediction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4aea5c14-ea3a-4e54-b9b0-aeafe1993269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b8f895ae944793abd1b4710b6b9b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict(df):\n",
    "\n",
    "    # One note at a time...\n",
    "    for row in tqdm(df.itertuples(), total=df.shape[0]):\n",
    "        \n",
    "        # Tokenize the entire discharge note\n",
    "        tokenized = cer_tokenizer(\n",
    "            row.text, \n",
    "            return_offsets_mapping=False,    \n",
    "            add_special_tokens=False,   \n",
    "            truncation=False,       \n",
    "        )\n",
    "\n",
    "        global_offset = 0\n",
    "        global_offset_mapping = []\n",
    "\n",
    "        # Adjust the token offsets so that they match the original document \n",
    "        for input_id in tokenized[\"input_ids\"]:\n",
    "            token = cer_tokenizer.decode(input_id)\n",
    "            pos = row.text[global_offset:].find(token)\n",
    "            start = global_offset + pos\n",
    "            end = global_offset + pos + len(token)\n",
    "            global_offset = end\n",
    "            global_offset_mapping.append((start, end))     \n",
    "\n",
    "        chunk_start_idx = 0\n",
    "        chunk_end_idx = 0            \n",
    "            \n",
    "        # Process the document in chunks of 512 tokens chunk at a time\n",
    "        for offset_chunk in chunked(global_offset_mapping, max_seq_len-2):\n",
    "            chunk_start_idx = chunk_end_idx\n",
    "            chunk_end_idx = offset_chunk[-1][1]\n",
    "            chunk_text = row.text[chunk_start_idx:chunk_end_idx]\n",
    "\n",
    "            # Iterate through the detected entities and link them\n",
    "            for entity in cer_pipeline(chunk_text):\n",
    "                example = pd.Series({\n",
    "                    # +1 to account for the [CLS] token\n",
    "                    \"start\": entity[\"start\"] + chunk_start_idx + 1, \n",
    "                    \"end\": entity[\"end\"] + chunk_start_idx,            \n",
    "                    \"text\": row.text\n",
    "                })\n",
    "                sctid = linker.link(example)\n",
    "\n",
    "                # Only yield matches where the Linker returned something\n",
    "                if sctid:\n",
    "                    yield {\n",
    "                        'note_id': row.Index,\n",
    "                        'start': example[\"start\"],  \n",
    "                        'end': example[\"end\"],\n",
    "                        'concept_id': sctid,\n",
    "                        # The following are useful for debugging and analysis\n",
    "                        'FSN': SG.get_concept_details(sctid).fsn,\n",
    "                        'entity': row.text[example[\"start\"]:example[\"end\"]],\n",
    "                        'tokenizer_word': entity[\"word\"]\n",
    "                    }\n",
    "\n",
    "preds_df = pd.DataFrame(list(predict(test_notes_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba937fd-5059-47a1-8831-638a3820bf5c",
   "metadata": {},
   "source": [
    "## 4.3 Visualisation\n",
    "\n",
    "The following code will compare the ground truth (\"GT_\") annotations to the predicted (\"P_\") annotations.  Since we cannot share the text of these notes, the outputs of this code have been hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a7017-cf34-496d-bb94-ea58902184ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_id = \"10807423-DS-19\"\n",
    "text = test_notes_df.loc[note_id].text\n",
    "\n",
    "predicted_annotations = [\n",
    "    (row.start, row.end, f'P_{row.concept_id}')\n",
    "    for row in preds_df.set_index(\"note_id\").loc[note_id].itertuples()\n",
    "]\n",
    "\n",
    "gt_annotations = [\n",
    "    (row.start, row.end, f'GT_{row.concept_id}')\n",
    "    for row in test_annotations_df.loc[note_id].itertuples()\n",
    "]\n",
    "\n",
    "show_span_line_markup(text, predicted_annotations + gt_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd7182-f98b-459a-af1f-57d4885e909f",
   "metadata": {},
   "source": [
    "## 4.3 Scoring\n",
    "\n",
    "We apply a token-level scorer function, which is what the competition will use to evaluate solutions.  We run this over our reserved test set to get a sense for out-of-sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47a8cd6d-808c-4af2-a137-2e81097ef10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_per_class(user_annotations: pd.DataFrame, target_annotations: pd.DataFrame) -> List[float]:\n",
    "    \"\"\"\n",
    "    Calculate the IoU metric for each class in a set of annotations.\n",
    "    \"\"\"\n",
    "    # Get mapping from note_id to index in array\n",
    "    docs = np.unique(np.concatenate([user_annotations.note_id, target_annotations.note_id]))\n",
    "    doc_index_mapping = dict(zip(docs, range(len(docs))))\n",
    "\n",
    "    # Identify union of categories in GT and PRED\n",
    "    cats = np.unique(np.concatenate([user_annotations.concept_id, target_annotations.concept_id]))\n",
    "\n",
    "    # Find max character index in GT or PRED\n",
    "    max_end = np.max(np.concatenate([user_annotations.end, target_annotations.end]))\n",
    "\n",
    "    # Populate matrices for keeping track of character class categorization\n",
    "    def populate_char_mtx(n_rows, n_cols, annot_df):\n",
    "        mtx = sp.lil_array((n_rows, n_cols), dtype=np.uint64)\n",
    "        for row in annot_df.itertuples():\n",
    "            doc_index = doc_index_mapping[row.note_id]\n",
    "            mtx[doc_index, row.start : row.end] = row.concept_id  # noqa: E203\n",
    "        return mtx.tocsr()\n",
    "\n",
    "    gt_mtx = populate_char_mtx(docs.shape[0], max_end, target_annotations)\n",
    "    pred_mtx = populate_char_mtx(docs.shape[0], max_end, user_annotations)\n",
    "\n",
    "    # Calculate IoU per category\n",
    "    ious = []\n",
    "    for cat in cats:\n",
    "        gt_cat = gt_mtx == cat\n",
    "        pred_cat = pred_mtx == cat\n",
    "        # sparse matrices don't support bitwise operators, but the _cat matrices\n",
    "        # have bool dtypes so when we multiply/add them we end up with only T/F values\n",
    "        intersection = gt_cat * pred_cat\n",
    "        union = gt_cat + pred_cat\n",
    "        iou = intersection.sum() / union.sum()\n",
    "        ious.append(iou)\n",
    "\n",
    "    return ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1ce6058-18c0-4596-a9d3-c49ac2dafd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-averaged character IoU metric: 0.2200\n"
     ]
    }
   ],
   "source": [
    "ious = iou_per_class(preds_df, test_annotations_df.reset_index())\n",
    "print(f\"macro-averaged character IoU metric: {np.mean(ious):0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e36e2e-2783-496d-a89a-a6f8fbd36548",
   "metadata": {},
   "source": [
    "# 5. Preparing for Submission\n",
    "\n",
    "Here we wrap the model up into a compliant submission format. (Note that, before submitting, we'd want to re-fit both the CER model (using the optimal number of training epochs) and the Linker on _all_ of the data.)\n",
    "\n",
    "Before we do so, it's a good idea to re-train the entity linker on all of the available notes, just to squeeze out every last drop of performance.\n",
    "\n",
    "The contents of `solution.py` are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3503fadf-1d7c-467c-aa94-e0dbb48056fe",
   "metadata": {},
   "source": [
    "## 5.1 Finalise the CER model\n",
    "\n",
    "We'll give it two epochs of supervised fine-tuning over the held-out notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9937cc3e-d85d-4fa6-b7a2-2c50084b7f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 08:30, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.165700</td>\n",
       "      <td>0.125090</td>\n",
       "      <td>0.837437</td>\n",
       "      <td>0.864745</td>\n",
       "      <td>0.850872</td>\n",
       "      <td>0.954326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.128800</td>\n",
       "      <td>0.114679</td>\n",
       "      <td>0.832992</td>\n",
       "      <td>0.882987</td>\n",
       "      <td>0.857261</td>\n",
       "      <td>0.956375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args.num_train_epochs = 2\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=cer_model,\n",
    "    args=training_args,\n",
    "    train_dataset=test,\n",
    "    eval_dataset=test,\n",
    "    tokenizer=cer_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"cer_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318b7384-4aa8-4e26-a76e-3830797fa1a7",
   "metadata": {},
   "source": [
    "## 5.2 Finalise the Linker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d0dbc5a-a5a5-42c2-9f18-6b02417089a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371c26fafd864038a4532374fa2ad0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kb_model = SentenceTransformer(\"kb_model\")\n",
    "linker = Linker(kb_model, 10)\n",
    "linker.fit(notes_df.join(annotations_df), concepts_in_scope)\n",
    "\n",
    "with open(\"linker.pickle\", \"wb\") as f:\n",
    "    pickle.dump(linker, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78967b52-16ff-4a25-a744-1cf5f95c7d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Benchmark submission for Entity Linking Challenge.\"\"\"\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "from more_itertools import chunked\n",
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import (\n",
    "    DebertaV2ForTokenClassification, AutoTokenizer, pipeline\n",
    ")\n",
    "import dill as pickle\n",
    "\n",
    "NOTES_PATH = Path(\"data/test_notes.csv\")\n",
    "SUBMISSION_PATH = Path(\"submission.csv\")\n",
    "LINKER_PATH = Path(\"linker.pickle\")\n",
    "CER_MODEL_PATH = Path(\"cer_model\")\n",
    "\n",
    "CONTEXT_WINDOW_WIDTH = 20\n",
    "MAX_SEQ_LEN = 512\n",
    "USE_LORA = False\n",
    "\n",
    "def load_cer_pipeline():\n",
    "\n",
    "    label2id = {\n",
    "        'O': 0, \n",
    "        'B-clinical_entity': 1, \n",
    "        'I-clinical_entity': 2\n",
    "    }    \n",
    "\n",
    "    id2label = {v: k for k,v in label2id.items()}\n",
    "\n",
    "    cer_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        CER_MODEL_PATH, model_max_length=MAX_SEQ_LEN\n",
    "    )\n",
    "\n",
    "    if USE_LORA:\n",
    "        config = PeftConfig.from_pretrained(CER_MODEL_PATH)\n",
    "\n",
    "        cer_model = DebertaV2ForTokenClassification.from_pretrained(\n",
    "            pretrained_model_name_or_path=config.base_model_name_or_path,\n",
    "            num_labels=3, \n",
    "            id2label=id2label, \n",
    "            label2id=label2id\n",
    "        )  \n",
    "        cer_model = PeftModel.from_pretrained(cer_model, CER_MODEL_PATH)\n",
    "    else:\n",
    "        cer_model = DebertaV2ForTokenClassification.from_pretrained(\n",
    "            pretrained_model_name_or_path=CER_MODEL_PATH,\n",
    "            num_labels=3, \n",
    "            id2label=id2label, \n",
    "            label2id=label2id\n",
    "        )  \n",
    "\n",
    "    cer_pipeline = pipeline(\n",
    "        task=\"token-classification\", \n",
    "        model=cer_model, \n",
    "        tokenizer=cer_tokenizer, \n",
    "        aggregation_strategy=\"first\",\n",
    "        device=\"cpu\"\n",
    "    )  \n",
    "    return cer_pipeline\n",
    "\n",
    "\n",
    "def main():    \n",
    "    # columns are note_id, text\n",
    "    logger.info(\"Reading in notes data.\")\n",
    "    notes = pd.read_csv(NOTES_PATH)\n",
    "    logger.info(f\"Found {notes.shape[0]} notes.\")\n",
    "    spans = []\n",
    "\n",
    "    # Load model components\n",
    "    logger.info(\"Loading CER pipeline.\")\n",
    "    cer_pipeline = load_cer_pipeline()\n",
    "    cer_tokenizer = cer_pipeline.tokenizer\n",
    "    \n",
    "    logger.info(\"Loading Linker\")\n",
    "    with open(LINKER_PATH, \"rb\") as f:\n",
    "        linker = pickle.load(f)\n",
    "    \n",
    "    # Process one note at a time...\n",
    "    logger.info(\"Processing notes.\")\n",
    "    for row in notes.itertuples():\n",
    "\n",
    "        # Tokenize the entire discharge note\n",
    "        tokenized = cer_tokenizer(\n",
    "            row.text, \n",
    "            return_offsets_mapping=False,    \n",
    "            add_special_tokens=False,       \n",
    "            truncation=False,       \n",
    "        )\n",
    "\n",
    "        global_offset = 0\n",
    "        global_offset_mapping = []\n",
    "\n",
    "        # Adjust the token offsets so that they match the original document \n",
    "        for input_id in tokenized[\"input_ids\"]:\n",
    "            token = cer_tokenizer.decode(input_id)\n",
    "            pos = row.text[global_offset:].find(token)\n",
    "            start = global_offset + pos\n",
    "            end = global_offset + pos + len(token)\n",
    "            global_offset = end\n",
    "            global_offset_mapping.append((start, end))     \n",
    "\n",
    "        chunk_start_idx = 0\n",
    "        chunk_end_idx = 0            \n",
    "            \n",
    "        # Process the document in chunks of 512 tokens chunk at a time\n",
    "        for offset_chunk in chunked(global_offset_mapping, MAX_SEQ_LEN-2):\n",
    "            chunk_start_idx = chunk_end_idx\n",
    "            chunk_end_idx = offset_chunk[-1][1]\n",
    "            chunk_text = row.text[chunk_start_idx:chunk_end_idx]\n",
    "\n",
    "            # ...one matched clinical entity at a time\n",
    "            # Iterate through the detected entities and link them\n",
    "            for entity in cer_pipeline(chunk_text):\n",
    "                example = pd.Series({\n",
    "                    # +1 to account for the [CLS] token\n",
    "                    \"start\": entity[\"start\"] + chunk_start_idx + 1, \n",
    "                    \"end\": entity[\"end\"] + chunk_start_idx,            \n",
    "                    \"text\": row.text\n",
    "                })\n",
    "                sctid = linker.link(example)\n",
    "                if sctid:\n",
    "                    spans.append({\n",
    "                        'note_id': row.Index,\n",
    "                        'start': example[\"start\"],\n",
    "                        'end': example[\"end\"],\n",
    "                        'concept_id': sctid\n",
    "                    })\n",
    "    \n",
    "    logger.info(f\"Generated {len(spans)} annotated spans.\")\n",
    "    spans_df = pd.DataFrame(spans)\n",
    "    spans_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "    logger.info(\"Finished.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe864ee-e7aa-4f2b-ae8a-48a2e0689bca",
   "metadata": {},
   "source": [
    "# Parting Words\n",
    "\n",
    "There's a fair amount that goes into an entity linker.  The approach we took here - using transformer encoders - has the virtue of being quick to fine-tune and easy to experiment with; on the flip-side, it's difficult to get good performance from a 300M parameter encoder for the CER step using \"out of the box\" fine-tuning.  Furthermore, the requirement to chunk the documents and align the annotations with the tokenization scheme adds unwelcome complexity to the code. Entity linkers that use modern, decoder-based transformers - having the virtue of longer context windows and a deeper \"understanding\" of natural language - should be able to beat this benchmark.\n",
    "\n",
    "Furthermore, the model constructed in notebook does not take full advantage of the knowledge encoded within the SNOMED Clinical Terminology.  We used synonyms to fine-tune the Knowledge Base Encoder but made no use of either the hierarchy or the defining relationships in constructing fine-tuning datasets. For example, in a decoder-based model, we can imagine developing _retrieval augmented generation_ techniques for candidate selection.\n",
    "\n",
    "The full power of SNOMED CT is an underexplored area for the development of Clinical Entity Linking models.  We wish you all the best in your experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096eea7-4a30-47d9-b14d-4a62d8a0f855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
